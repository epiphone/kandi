% !TeX encoding = utf8
%DIF LATEXDIFF DIFFERENCE FILE
%DIF DEL tp2/kandiTP2-23-11.tex   Thu Nov 21 21:29:58 2013
%DIF ADD kandi_tyoversio.tex      Tue Dec  3 16:27:57 2013
%
% [ Tiedostossa käytetty merkistö on utf8, vaihtoehtoisesti voisi olla esim.]
% [ ISO 8859-1 eli Latin 1. Ylläoleva rivi ]
% [ tarvitaan, jos käyttää MiKTeX-paketin mukana tulevaa TeXworks-editoria. ]
%
% TIETOTEKNIIKAN KANDIDAATINTUTKIELMA
%
% Yksinkertainen LaTeX2e-mallipohja kandidaatintutkielmalle.
% Käyttää Antti-Juhani Kaijanahon kirjoittamaa gradu3-dokumenttiluokkaa.
%
% Jos kirjoitat pro gradu -tutkielmaa, tee mallipohjaan seuraavat muutokset:
%  - Poista dokumenttiluokasta optio bachelor .
%  - Poista makro \type .
%  - Lisää suuntautumisvaihtoehto makrolla \studyline .
%  - Lisää tieto ohjaajasta makrolla \supervisor .

\documentclass[utf8,bachelor,manualbib]{gradu3}

\usepackage{palatino} % valitaan oletusfonttia hieman tyylikkäämpi fontti

\usepackage{graphicx} % tarvitaan vain, jos halutaan mukaan kuvia
\usepackage{amsmath}  % tarvitaan käytettäessä monimutkaisten matemaattisten kaavojen ja \eqref-kaavaviittauksen yhteydessä
\usepackage{url} % tarvitaan \url-komentoa varten
\usepackage{booktabs}

\usepackage{amsfonts}
\usepackage{enumitem}
%DIF 29a29-31
\usepackage{eucal}
 %DIF > 
\usepackage{pbox}
 %DIF > 
\usepackage{comment}
 %DIF > 
%DIF -------

% Otetaan käyttöön author-date-järjestelmän mukaiset lähdeviittaukset:
\usepackage{natbib}
% Vaihdetaan kirjoittajan nimen ja vuosiluvun väliseksi erottimeksi
% välilyönti (oletuserottimena on pilkku):
%\bibpunct{(}{)}{;}{a}{}{,}

\newcommand{\argmax}[1]{\underset{#1}{\operatorname{arg}\,\operatorname{max}}\;}

% HUOM! Tämän tulee olla viimeinen \usepackage koko dokumentissa!
\usepackage[bookmarksopen,bookmarksnumbered,linktocpage]{hyperref}
%DIF < 
%DIF -------
\setcounter{tocdepth}{2}
 %DIF > 
%DIF < %\addbibresource{viite.bib}% Lähdetietokannan tiedostonimi
%DIF < %http://www.tex.ac.uk/tex-archive/macros/latex/exptl/biblatex-contrib/biblatex-chicago/latex/biblatex-chicago.sty
%DIF < %http://www.tex.ac.uk/tex-archive/macros/latex/contrib/etoolbox/etoolbox.sty
%DIF < %http://mirrors.med.harvard.edu/ctan/macros/latex/contrib/biblatex/latex/biblatex.sty
%DIF < %http://ctan.mackichan.com/macros/latex/contrib/biblatex/latex/biblatex2.sty
%DIF < %http://mirror.hmc.edu/ctan/macros/latex/contrib/logreq/logreq.sty
%DIF < %https://github.com/Martin-Rotter/qt-survival-guide/blob/master/logreq.def
%DIF < 
%DIF PREAMBLE EXTENSION ADDED BY LATEXDIFF
%DIF UNDERLINE PREAMBLE %DIF PREAMBLE
\RequirePackage[normalem]{ulem} %DIF PREAMBLE
\RequirePackage{color}\definecolor{RED}{rgb}{1,0,0}\definecolor{BLUE}{rgb}{0,0,1} %DIF PREAMBLE
\providecommand{\DIFaddtex}[1]{{\protect\color{blue}\uwave{#1}}} %DIF PREAMBLE
\providecommand{\DIFdeltex}[1]{{\protect\color{red}\sout{#1}}}                      %DIF PREAMBLE
%DIF SAFE PREAMBLE %DIF PREAMBLE
\providecommand{\DIFaddbegin}{} %DIF PREAMBLE
\providecommand{\DIFaddend}{} %DIF PREAMBLE
\providecommand{\DIFdelbegin}{} %DIF PREAMBLE
\providecommand{\DIFdelend}{} %DIF PREAMBLE
%DIF FLOATSAFE PREAMBLE %DIF PREAMBLE
\providecommand{\DIFaddFL}[1]{\DIFadd{#1}} %DIF PREAMBLE
\providecommand{\DIFdelFL}[1]{\DIFdel{#1}} %DIF PREAMBLE
\providecommand{\DIFaddbeginFL}{} %DIF PREAMBLE
\providecommand{\DIFaddendFL}{} %DIF PREAMBLE
\providecommand{\DIFdelbeginFL}{} %DIF PREAMBLE
\providecommand{\DIFdelendFL}{} %DIF PREAMBLE
%DIF END PREAMBLE EXTENSION ADDED BY LATEXDIFF
%DIF PREAMBLE EXTENSION ADDED BY LATEXDIFF
%DIF HYPERREF PREAMBLE %DIF PREAMBLE
\providecommand{\DIFadd}[1]{\texorpdfstring{\DIFaddtex{#1}}{#1}} %DIF PREAMBLE
\providecommand{\DIFdel}[1]{\texorpdfstring{\DIFdeltex{#1}}{}} %DIF PREAMBLE
%DIF END PREAMBLE EXTENSION ADDED BY LATEXDIFF

\begin{document}

\title{Sanaluokkien automaattisen tunnistamisen menetelmät}

\translatedtitle{Methods for automated part-of-speech tagging}

%\studyline{}
\DIFdelbegin %DIFDELCMD < \avainsanat{luonnollisten kielten käsittely, sanaluokkien tunnistaminen, koneoppiminen}
%DIFDELCMD < \keywords{natural language processing, part-of-speech tagging, machine learning}
%DIFDELCMD < %%%
\DIFdelend \DIFaddbegin \avainsanat{kieliteknologia, luonnollisten kielten käsittely, sanaluokkien tunnistaminen, koneoppiminen}
\keywords{computational linguistics, natural language processing, part-of-speech tagging, machine learning}
\DIFaddend \tiivistelma{Tiivistelmä on tyypillisesti 5-10 riviä pitkä esitys työn pääkohdista (tausta, tavoite, tulokset, johtopäätökset).
}
\abstract{Englanninkielinen versio tiivistelmästä.
}

\author{Aleksi Pekkala}
\contactinformation{\texttt{aleksi.v.a.pekkala@student.jyu.fi}}
% jos useita tekijöitä, anna useampi \author-komento
%\supervisor{Kirsi Valjus}
% jos useita ohjaajia, anna useampi \supervisor-komento
%\type{kandidaatintutkielma} % tämän makron oletus on "pro gradu -tutkielma" ja bachelor-optiolla kandidaatintutkielma

\maketitle

\mainmatter


\chapter{Johdanto}


Sanaluokkien automaattinen tunnistaminen (engl. \emph{part-of-speech tagging}) tarkoittaa sanojen sanaluokkien tunnistamista tekstiyhteyden perusteella.  Tunnistamisprosessissa tarkastellaan tekstiaineistoa, kuten

\[a\:black\:cat\:jumped\:on\:the\:table\]

jonka perusteella pyritään päättelemään se sanaluokkien sarja, joka todennäköisimmin vastaa kyseistä aineistoa; tässä tapauksessa tuloksena voisi olla esimerkiksi

\[Det\:Adj\:Noun\:Verb\:Prep\:Det\:Noun\]

Sanaluokkien tunnistaminen on laajuudeltaan rajallinen ongelma: sen tarkoituksena ei ole jäsentää kokonaisia lauserakenteita tai tulkita lauseiden merkitystä --- tarkastelun alla ovat vain yksittäisten sanojen \DIFdelbegin \DIFdel{syntaktiset }\DIFdelend \DIFaddbegin \DIFadd{leksikaaliset }\DIFaddend kategoriat. Sanaluokkien tunnistaminen on kuitenkin välttämätön ensimmäinen askel useimmissa luonnollisten kielten käsittelyprosesseissa, ja siten yksi aihealueen keskeisimmistä osaongelmista.

Rajallisen laajuutensa myötä sanaluokkien tunnistaminen on paljon helpommin lähestyttävä ongelma kuin kielen täydellinen ymmärtäminen, ja sen ratkaisemiseksi onkin kehitetty useita kohtuullisen luotettavia menetelmiä. Täysin ratkaistusta ongelmasta ei kuitenkaan voida puhua, sillä yksikään tunnettu menetelmä ei vielä saavuta täydellistä tunnistustarkkuutta.

Sanaluokkien tunnistajia käytetään monissa erilaisissa luonnollisiin kieliin liittyvissä sovelluksissa, ja tunnistajalle asetetut vaatimukset vaihtelevat sovelluksittain. Myös tunnistettavien aineistojen välillä on valtavasti poikkeamia, esim. kielten sekä tekstilajien osalta. Lisäksi havaitaan, että nykyisten tunnistusmenetelmien saavuttamat tunnistustarkkuudet liikkuvat kaikki suunnilleen samoissa lukemissa. Kun ilmiselvin valintakriteeri on näin poissuljettu, on tehokkaimman menetelmän valinta vaikeampaa. Tunnistusmenetelmien toimintaperiaatteiden vaihdellessa merkittävästi on kuitenkin väistämätöntä, että jotkin menetelmät soveltuvat toisia paremmin tiettyihin tunnistustehtäviin. Tässä tutkielmassa pyritäänkin selventämään sitä, \DIFdelbegin \DIFdel{kuinka eri tunnistusmenetelmät käyttäytyvät suhteessa toisiinsa erilaisissa toimintaympäristöissä, }\DIFdelend \DIFaddbegin \DIFadd{millaisia eri ratkaisuja sanaluokkien tunnistusongelmaan on olemassa }\DIFaddend ja mitkä ovat niiden \DIFdelbegin \DIFdel{oleellisimmat vahvuudet sekä}\DIFdelend \DIFaddbegin \DIFadd{tä}\DIFaddend ä\DIFdelbegin \DIFdel{heikkoudet}\DIFdelend \DIFaddbegin \DIFadd{rkeimmät erot}\DIFaddend . Menetelmien suhteelliset ominaisuudet johdetaan tarkastelemalla lähemmin kunkin menetelmän toimintaa \DIFdelbegin \DIFdel{, }\DIFdelend sekä menetelmään liittyvää tutkimuskirjallisuutta. 

Tutkielma rakentuu seuraavasti: toisessa luvussa annetaan lyhyt johdanto sanaluokkien tunnistamiseen ja sen haasteisiin. Luvuissa 3-5 tarkastellaan kolmea erilaista tunnistusmenetelmää\DIFdelbegin \DIFdel{, }\DIFdelend \DIFaddbegin \DIFadd{: transformaatiosääntöjä, Markovin piilomalleja sekä log-lineaarisia malleja. Luvuissa }\DIFaddend esitellään \DIFdelbegin \DIFdel{niiden }\DIFdelend \DIFaddbegin \DIFadd{menetelmien }\DIFaddend keskeiset ominaisuudet, \DIFdelbegin \DIFdel{ja }\DIFdelend \DIFaddbegin \DIFadd{sekä }\DIFaddend pyritään hahmottamaan \DIFdelbegin \DIFdel{kunkin menetelmä}\DIFdelend \DIFaddbegin \DIFadd{niiden suhteelliset vahvuudet ja heikkoudet. Lopuksi vielä}\DIFaddend ä \DIFdelbegin \DIFdel{n suhteelliset vahvuudet . TODO mainitaan }\DIFdelend \DIFaddbegin \DIFadd{kootaan yhteen menetelmistä kerätyt huomiot ja esitellään johtopäätökset.

}

\DIFadd{TODO olisiko syytä esitellä }\DIFaddend lyhyesti valitut menetelmät \DIFdelbegin \DIFdel{ja valintaperusteet (}\DIFdelend \DIFaddbegin \DIFadd{tässä, ja/tai mainita jotain valintaperusteista tai }\DIFaddend esitysjärjestyksestä \DIFdelbegin \DIFdel{: }\DIFdelend \DIFaddbegin \DIFadd{(}\DIFaddend kaksi ensimmäistä ovat tavallaan toistensa vastakohtia, ja kolmas menetelmä yhdistää piirteitä kummastakin aikaisemmasta menetelmästä. Samalla menetelmät ovat järjestetty yksinkertaisimmasta monimutkaisimpaan.).


\chapter{Sanaluokkien automaattinen tunnistaminen}

\DIFdelbegin \section{\DIFdel{Mihin sanaluokkien tunnistamista käytetään?}}

%DIFAUXCMD
\addtocounter{section}{-1}%DIFAUXCMD
%DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdelend Sanaluokkien tunnistaminen on tärkeä ja käytännöllinen ongelma, joka kohdataan \DIFdelbegin \DIFdel{useimmissa }\DIFdelend \DIFaddbegin \DIFadd{lähes kaikissa }\DIFaddend luonnollisten kielten käsittelyyn liittyvissä tehtävissä. Tällaisia tehtäviä ovat mm. puheentunnistus, konekääntäminen sekä semanttinen haku ja analyysi. Kyseisissä tehtävissä sanaluokkien tunnistaja toimii jonkin laajemman prosessointiketjun alkupäässä, koko prosessille välttämättömänä esikäsittelijänä, joka mahdollistaa syötteen jatkokäsittelyn korkeammalla tasolla. Jatkokäsittelyn tyypillisin seuraava vaihe on tekstin jäsentäminen eli lauserakenteiden tunnistaminen. Oikeiden lauserakenteiden tunnistamisen kannalta on oleellista, että lauseiden sanaluokat on tunnistettu mahdollisimman virheettömästi: yksikin virheellinen sanaluokka voi tehdä oikean lauserakenteen tunnistamisesta mahdotonta, ja siten vääristää lauseen tulkittua merkitystä.


\DIFdelbegin \section{\DIFdel{Sanaluokkien tunnistamisen lyhyt historia}}

%DIFAUXCMD
\addtocounter{section}{-1}%DIFAUXCMD
%DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdel{TODO

}%DIFDELCMD < 

%DIFDELCMD < 
%DIFDELCMD < %%%
\DIFdelend \section{Miksi sanaluokkien tunnistaminen on ongelmallista?}

Sanaluokkien tunnistaminen voi intuitiivisesti tuntua helpolta, mutta tehtävän automaatiota hankaloittavat kaksi oleellista ongelmaa: sanojen monitulkintaisuus sekä tuntemattomat sanat. Esimerkiksi \DIFdelbegin \DIFdel{lauseet
}\DIFdelend \DIFaddbegin \DIFadd{lauseissa
}\DIFaddend \[Time\: flies\: like\: an\: arrow\DIFaddbegin \DIFadd{.}\DIFaddend \]
\[Fruit\: flies\: like\: a\: banana\DIFdelbegin %DIFDELCMD < \]
%DIFDELCMD < %%%
\DIFdel{voidaan tulkita lukuisin eri tavoin, joista mikä}\DIFdelend \DIFaddbegin \DIFadd{.}}
\DIFadd{sana }\textit{\DIFadd{flies}} \DIFadd{esiintyy ensin verbinä}\DIFaddend ä \DIFaddbegin \DIFadd{ja sitten substantiivina. Sanan }\textit{\DIFadd{time}} \DIFadd{ilmeisin sanaluokka on substantiivi, mutta se voidaan mieltä}\DIFaddend ä\DIFdelbegin \DIFdel{än ei ole vä}\DIFdelend ä \DIFdelbegin \DIFdel{lttä}\DIFdelend \DIFaddbegin \DIFadd{myös imperatiiviverbinä}\DIFaddend ä\DIFdelbegin \DIFdel{mä}\DIFdelend \DIFaddbegin \DIFadd{, jolloin lauseen merkitys muuttuu tä}\DIFaddend ä\DIFdelbegin \DIFdel{ttä}\DIFdelend \DIFaddbegin \DIFadd{ysin. Itse asiassa kummatkin esimerkkilauseet voidaan tulkita kymmenin eri tavoin, joista ilmeisimmä}\DIFaddend ä\DIFdelbegin \DIFdel{muita ilmeisempi}\DIFdelend \DIFaddbegin \DIFadd{n tulkinnan valitseminen automaattisesti on haastavaa}\DIFaddend . Lisäksi, vaikka tosielämässä tulkittavat lauseet ovat harvoin yhtä ongelmallisia kuin edellämainitut lingvistiset esimerkkilauseet, on monitulkintaisuus hyvin yleistä: arviolta 40\% englanninkielisen proosan sanastosta \DIFdelbegin \DIFdel{omaa useamman kuin yhden merkityksen \mbox{%DIFAUXCMD
\citep{derose1988}
}%DIFAUXCMD
. 

}\DIFdelend \DIFaddbegin \DIFadd{voidaan luokitella useampaan kuin yhteen sanaluokkaan \mbox{%DIFAUXCMD
\citep{derose1988}
}%DIFAUXCMD
. 

}

\DIFaddend Jatkokäsittelyn kannalta automaattisen tunnistajan oleellisin tehtävä onkin \DIFdelbegin \DIFdel{sen sopivimman merkityksen valinta, eli ns. morfologinen }\DIFdelend \DIFaddbegin \DIFadd{valita kaikista mahdollisista sanaluokista se, joka tuottaa luontevimman tulkinnan. Tällaisen }\DIFaddend yksikäsitteistä\DIFdelbegin \DIFdel{minen. Yksikäsitteistä}\DIFdelend misen mahdollistavat luonnollisten kielten sisäänrakennetut rajoitteet, \DIFdelbegin \DIFdel{ja erityisesti kaksi oleellista vihjetyyppiä}\DIFdelend \DIFaddbegin \DIFadd{jotka voidaan jakaa lokaaleihin sekä}\DIFaddend ä \DIFdelbegin \DIFdel{: lokaalit vihjeet }\DIFdelend \DIFaddbegin \DIFadd{kontekstuaalisiin vihjeisiin: lokaaleista vihjeistä ilmeisin on itse sana }\DIFaddend (''sana \textit{can} on \DIFaddbegin \DIFadd{on }\DIFaddend todennäköisemmin modaaliverbi kuin substantiivi''), \DIFdelbegin \DIFdel{sekä}\DIFdelend \DIFaddbegin \DIFadd{mutta pä}\DIFaddend ä\DIFdelbegin \DIFdel{kontekstuaaliset vihjeet (''}\DIFdelend \DIFaddbegin \DIFadd{ätelmiä voidaan tehdä myös mm. sanan prefiksin, suffiksin tai kirjainten koon perusteella. Kontekstuaalisia vihjeitä ovat kaikki lauseen muut sanat sanaluokkineen: esimerkiksi }\DIFaddend sana \textit{fly} on todennäköisimmin substantiivi, jos edeltävä sana on artikkeli\DIFdelbegin \DIFdel{'')}\DIFdelend .

\DIFdelbegin \DIFdel{Toinen }\DIFdelend \DIFaddbegin \DIFadd{On tärkeää huomata, ettei sanaluokkien tunnistaminen itsessään ole ratkaisu kieliopilliseen monitulkintaisuuteen: monitulkintaisuudella on useita tasoja, joista osaa käsitellään vielä prosessointiketjun myöhemmissä vaiheissa. Esimerkiksi syntaktinen, tai rakenteellinen monitulkintaisuus on ongelma, joka on huomattavasti helpompi ratkaista lauseiden jäsennyksen yhteydessä. Sanaluokkien tunnistamista ei myöskään tule sekoittaa semanttiseen yksikäsitteistämiseen, eli sanan merkityksen selvittämiseen: esimerkiksi sana }\textit{\DIFadd{mouse}} \DIFadd{on semanttisesti monitulkintainen, vaikka sen sanaluokka tunnettaisiinkin. Sanaluokkien tunnistamisen rooli on pikemminkin rajata mahdollisten tulkintojen määrää prosessointiketjun alkupäässä, jotta myöhemmissä vaiheissa vältytään turhalta työltä.

}

\DIFadd{Monitulkintaisuuden lisäksi toinen }\DIFaddend merkittävä ongelma on tuntemattomien, eli harjoitusaineistosta puuttuvien sanojen käsitteleminen. \DIFdelbegin \DIFdel{Tuntemattomia sanoja }\DIFdelend \DIFaddbegin \DIFadd{Englannin kielessä yleisiä tuntemattomia sanoja ovat erisnimet sekä puhekieliset, vieraskieliset ja muut harvinaiset ilmaisut. Tällaisia sanoja }\DIFaddend kohdataan usein, ja koska niitä koskevaa tilastollista informaatiota tai sääntöjä ei tunneta, joudutaan turvautumaan jonkinlaiseen poikkeuskäsittelyyn. Useat tunnistajat hyödyntävät tuntemattomien sanojen kohdalla kieliopillisia ominaisuuksia: yksinkertainen menetelmä on määrätä sanalle se sanaluokka, joihin tuntemattomien sanojen on havaittu todennäköisimmin kuuluvan, eli yleensä substantiivi. Parempia tuloksia on saavutettu määrittämällä tuntemattoman sanan sanaluokka sen päätteen perusteella; esim. englannin kielen \emph{able}-päätteiset sanat ovat hyvin todennäköisesti adjektiiveja \citep{samuelsson1993}. Menetelmä ei kuitenkaan sovellu kaikille kielille: esim. \citet{tseng2005} osoittavat, että kiinan kielessä esiintyy huomattavan suuri määrä yleisiä affikseja, poiketen englannin ja saksan kielistä, joissa affiksit ovat vahva indikaattori sanan sanaluokasta.


\DIFdelbegin \DIFdel{Ongelmallista on myös tunnistamisessa käytettävien sanaluokkien (engl. }\emph{\DIFdel{POS tagset}}%DIFAUXCMD
\DIFdel{) valinta. Yleisiä englannin kielen sanaluokkasettejä ovat Brownin aineiston 87 sanaluokkaa \mbox{%DIFAUXCMD
\citep{francis1964}
}%DIFAUXCMD
, tai uudemman Penn Treebank-aineiston 48 sanaluokkaa \mbox{%DIFAUXCMD
\citep{marcus1993}
}%DIFAUXCMD
. Myös yleisiä, kielestä riippumattomia sanaluokkasettejä on kehitetty, joskin tällöin joudutaan väistämättä tinkimään tunnistamistarkkuudesta \mbox{%DIFAUXCMD
\citep{petrov2011}
}%DIFAUXCMD
.

}%DIFDELCMD < 

%DIFDELCMD < 
%DIFDELCMD < %%%
\DIFdelend \section{Automaattisten tunnistajien suorituskyky}

Kuten mainittua, nykyisten automaattisten sanaluokkien tunnistajien tunnistustarkkuus --- englanninkielistä kirjakieltä analysoitaessa --- on hieman yli 97\%  (\DIFaddbegin \DIFadd{mm. }\DIFaddend \citealp{toutanova2003}, \citealp{shen2007}, \citealp{spoustova2009}\DIFdelbegin \DIFdel{, \mbox{%DIFAUXCMD
\citealp{sogaard2010}
}%DIFAUXCMD
}\DIFdelend ). \citet{manning2011} kuitenkin osoittaa, että kyseistä tulosta ei ole syytä tulkita liian optimistisesti: esimerkiksi lukuisat välikemerkit ja muut yksikäsitteiset elementit vääristävät evaluaatiotuloksia. Lisäksi useissa tekstilajeissa, kuten uutisiartikkeleissa, lauseiden keskipituus on yli 20 sanaa, jolloin edellämainitullakin tunnistustarkkuudella jokaisessa lauseessa on keskimäärin ainakin yksi virhe \citep{manning1999}. Artikkelissa huomautetaankin, että realistisempaa olisi tarkastella automaattisten tunnistajien kykyä tunnistaa kokonaiset lauseet oikein, sillä pienikin virhe lauseessa voi vahingoittaa tunnistajan hyödyllisyyttä myöhempien prosessointivaiheiden kannalta; tällä saralla tunnistajat saavuttavat noin 55-57\% tarkkuuden, mikä on huomattavasti vaatimattomampi tulos.

Tarkkuustuloksia arvioidessa tulee myös ottaa huomioon varsin korkea lähtötaso: jo yksinkertaisimmalla metodilla, eli valitsemalla kullekin sanalle se sanaluokka, joka esiintyy harjoitusaineistossa useiten annetun sanan yhteydessä, saavutetaan 90\% tunnistustarkkuus \citep{charniak1993}.

On myös mielenkiintoista huomata, että edes ammattilaisten käsin luokittelemat aineistot eivät saavuta täydellistä tunnistamistarkkuutta: ihmisten sanaluokkien tunnistamistarkkuuden on arvioitu olevan noin 97\% \citep{manning2011}, mikä vastaa edellämainittua automaattisten tunnistajien huipputulosta.


\section{Sanaluokkien tunnistajan vaatimukset}

Jotta tunnistajaa voidaan käyttää laajan kielenprosessointijärjestelmän komponenttina, tulee sen toteuttaa seuraavat ominaisuudet ~\citep{cutting1992}: 

\begin{description}[labelindent=1cm]
 \item[Kestävyys] Tunnistajan tulee kyetä selviytymään kaikista tekstisyötteen mahdollisista poikkeamista, kuten otsikoista, taulukoista sekä tuntemattomista sanoista.
 \item[Tehokkuus] Voidakseen käsitellä laajoja tekstiaineistoja tunnistajan tulee toimia lineaarisessa ajassa. 
 \item[Tarkkuus] Tunnistajan tulee kyetä ehdottamaan sanaluokka jokaista annettua sanaa kohden. Myös tunnistajan mahdollisen opettamisen tulisi onnistua mahdollisimman nopeasti.
 \item[Viritettävyys] Tunnistajan tulee osata hyödyntää erilaisia kielitieteellisiä huomioita siten, että tunnistajan tekemiä virheitä voidaan paikata määrittämällä sopivia vihjeitä.
 \item[Uudelleenkäytettävyys] Tunnistajan tulee rakentua siten, että sen kohdistaminen uudelle kielelle, aineistolle tai sanaluokkasetille on mahdollisimman vaivatonta.
\end{description}

\section{Harjoitusaineisto ja sanaluokkasetit}

\DIFdelbegin \DIFdel{TODO }\DIFdelend \DIFaddbegin \DIFadd{Sanaluokkien tunnistaminen on luonteeltaan sarjanluokitteluongelma, joka taas on yksi koneoppimisen (tarkemmin hahmontunnistuksen) aliongelmatyypeistä. }\DIFaddend Tä\DIFdelbegin \DIFdel{ssä}\DIFdelend \DIFaddbegin \DIFadd{mä}\DIFaddend ä\DIFdelbegin \DIFdel{kappaleessa mainitaan yleisimmä}\DIFdelend \DIFaddbegin \DIFadd{n seurauksena nykyiset sanaluokkien tunnistusmenetelmä}\DIFaddend ät \DIFdelbegin \DIFdel{harjoitusaineistot ja sanaluokkasetit (Brown, Penn Treebank). Mahd. syytä}\DIFdelend \DIFaddbegin \DIFadd{perustuvat lä}\DIFaddend ä\DIFdelbegin \DIFdel{tä}\DIFdelend \DIFaddbegin \DIFadd{hes poikkeuksetta ohjattuun oppimiseen, eli tunnistimet tulee ''opettaa'' jonkinlaisella harjoitusaineistolla ennen kä}\DIFaddend ä\DIFdelbegin \DIFdel{smentä}\DIFdelend \DIFaddbegin \DIFadd{yttöä. Sanaluokkien tunnistimille syötettä}\DIFaddend ä\DIFaddbegin \DIFadd{vä harjoitusdata koostuu suurista tekstiaineistoista (engl. }\textit{\DIFadd{corpus}}\DIFadd{), joissa jokaisen sanan yhteyteen on merkattu oikea sanaluokka. Nykyisin ehkä yleisimmin käytetty englanninkielinen harjoitusaineisto on ns. Penn Treebank-aineisto \mbox{%DIFAUXCMD
\citep{marcus1993}
}%DIFAUXCMD
, joka sisältä}\DIFaddend ä \DIFdelbegin \DIFdel{, että}\DIFdelend \DIFaddbegin \DIFadd{noin viiden miljoonan sanan edestä}\DIFaddend ä \DIFdelbegin \DIFdel{on kyse ohjatusta oppimisesta. Testiaineiston ja harjoitusaineiston erot; mitä}\DIFdelend \DIFaddbegin \DIFadd{uutisartikkeleita, kaunokirjallisuutta, tieteellisiä}\DIFaddend ä \DIFdelbegin \DIFdel{seuraa jos liian erilaiset? Ongelmaan vaikuttaa }\DIFdelend \DIFaddbegin \DIFadd{julkaisuja ym. tekstilajeja.

}

\DIFadd{TODO harjoitusaineiston merkitys tunnistustuloksille.

}

\DIFadd{Tunnistusongelmaan on olemassa }\DIFaddend myös \DIFdelbegin \DIFdel{tunnistettavien sanaluokkien }\DIFdelend \DIFaddbegin \DIFadd{ohjaamattomaan oppimiseen perustuvia ratkaisuja, jotka eivät vaadi valmiiksi luokiteltua harjoitusaineistoa. Toisaalta tällaiset menetelmät ovat väistä}\DIFaddend mä\DIFaddbegin \DIFadd{ttä alttiimpia virheille kuin vastaavat ohjatun oppimisen menetelmät. Joissain tapauksissa --- esimerkiksi harvinaisia kieliä analysoitaessa --- luokiteltua harjoitusaineistoa ei kuitenkaan ole saatavilla, jolloin ohjaamaton oppiminen on ainoa vaihtoehto.

}

\DIFadd{Tunnistamisessa käytetyt sanaluokat mä}\DIFaddend ärä\DIFdelbegin \DIFdel{: }\DIFdelend \DIFaddbegin \DIFadd{ytyvät yleensä harjoitusaineiston mukaan: esimerkiksi Penn Treebank-aineiston käyttämä sanaluokkasetti koostuu 48 sanaluokasta, joista 12 ovat välikemerkkejä. On tärkeää huomata, että käytettävän sanaluokkasetin laajuus vaikuttaa suoraan tunnistustarkkuuteen: }\DIFaddend mitä enemmän sanaluokkia, sitä suurempi mahdollisuus monitulkintaisuuteen. \DIFdelbegin \DIFdel{Harjoitusaineiston koko. }\DIFdelend \DIFaddbegin \DIFadd{Toisaalta sanaluokkien tunnistamisen tuottama lingvistinen informaatio on sitä arvokkaampaa, mitä tarkempi jaottelu eri sanaluokkien välillä on. \mbox{%DIFAUXCMD
\citep{marcus1993}
}%DIFAUXCMD
}\DIFaddend 

\DIFaddbegin \DIFadd{TODO mahdollisesti listaus Penn Treebank-sanaluokkasetistä, esimerkkipätkä harjoitusaineistosta.

}

%DIF >  Tagset-taulukko tällä hetkellä piilotettu
\begin{comment}
{\renewcommand{\arraystretch}{0.8}
\begin{table}[H]\footnotesize
  \caption{\DIFaddFL{Penn Treebank-sanaluokkasetti \mbox{%DIFAUXCMD
\citep{marcus1993}
}%DIFAUXCMD
}}
  \begin{tabular}{rllrll}
\DIFaddFL{1. }& \DIFaddFL{CC }& \DIFaddFL{Coordinating conjunction }& \DIFaddFL{25. }& \DIFaddFL{TO }& \DIFaddFL{to }\\
\DIFaddFL{2. }& \DIFaddFL{CD }& \DIFaddFL{Cardinal number }& \DIFaddFL{26. }& \DIFaddFL{UH }& \DIFaddFL{Interjection }\\
\DIFaddFL{3. }& \DIFaddFL{DT }& \DIFaddFL{Determiner }& \DIFaddFL{27. }& \DIFaddFL{VB }& \DIFaddFL{Verb, base form }\\
\DIFaddFL{4. }& \DIFaddFL{EX }& \DIFaddFL{Existential there }& \DIFaddFL{28. }& \DIFaddFL{VBD }& \DIFaddFL{Verb, past tense }\\
\DIFaddFL{5. }& \DIFaddFL{FW }& \DIFaddFL{Foreign word }& \DIFaddFL{29. }& \DIFaddFL{VBG }& \DIFaddFL{Verb, gerund/present }\\
\DIFaddFL{6. }& \DIFaddFL{IN }& \pbox{20cm}{Preposition/subordinating \\ participle conjunction} & \DIFaddFL{30. }& \DIFaddFL{VBN }& \DIFaddFL{Verb, past participle }\\
\DIFaddFL{7. }& \DIFaddFL{JJ }& \DIFaddFL{Adjective }& \DIFaddFL{31. }& \DIFaddFL{VBP }& \DIFaddFL{Verb, non-3rd ps. sing. present }\\
\DIFaddFL{8. }& \DIFaddFL{JJR }& \DIFaddFL{Adjective, comparative }& \DIFaddFL{32. }& \DIFaddFL{VBZ }& \DIFaddFL{Verb, 3rd ps. sing. present }\\
\DIFaddFL{9. }& \DIFaddFL{JJS }& \DIFaddFL{Adjective, superlative }& \DIFaddFL{33. }& \DIFaddFL{WDT }& \DIFaddFL{wh-determiner }\\
\DIFaddFL{10. }& \DIFaddFL{LS }& \DIFaddFL{List item marker }& \DIFaddFL{34. }& \DIFaddFL{WP }& \DIFaddFL{wh-pronoun }\\
\DIFaddFL{11. }& \DIFaddFL{MD }& \DIFaddFL{Modal }& \DIFaddFL{35. }& \DIFaddFL{WP\$ }& \DIFaddFL{Possessive wh-pronoun }\\
\DIFaddFL{12. }& \DIFaddFL{NN }& \DIFaddFL{Noun, singular or mass }& \DIFaddFL{36. }& \DIFaddFL{WRB }& \DIFaddFL{wh-adverb }\\
\DIFaddFL{13. }& \DIFaddFL{NNS }& \DIFaddFL{Noun, plural }& \DIFaddFL{37. }& \# & \DIFaddFL{Pound sign }\\
\DIFaddFL{14. }& \DIFaddFL{NNP }& \DIFaddFL{Proper noun, singular }& \DIFaddFL{38. }& \DIFaddFL{\$ }& \DIFaddFL{Dollar sign }\\
\DIFaddFL{15. }& \DIFaddFL{NNPS }& \DIFaddFL{Proper noun, plural  }& \DIFaddFL{39. }& \DIFaddFL{. }& \DIFaddFL{Sentence-final punctuation }\\
\DIFaddFL{16. }& \DIFaddFL{PDT }& \DIFaddFL{Predeterminer }& \DIFaddFL{40. }& \DIFaddFL{, }& \DIFaddFL{Comma }\\
\DIFaddFL{17. }& \DIFaddFL{POS }& \DIFaddFL{Possessive ending }& \DIFaddFL{41. }& \DIFaddFL{: }& \DIFaddFL{Colon, semi-colon }\\
\DIFaddFL{18. }& \DIFaddFL{PRP }& \DIFaddFL{Personal pronoun }& \DIFaddFL{42. }& \DIFaddFL{( }& \DIFaddFL{Left bracket character }\\
\DIFaddFL{19. }& \DIFaddFL{PP\$  }&\DIFaddFL{Possessive pronoun }& \DIFaddFL{43. }& \DIFaddFL{) }& \DIFaddFL{Right bracket character }\\
\DIFaddFL{20. }& \DIFaddFL{RB }& \DIFaddFL{Adverb }& \DIFaddFL{44. }& \DIFaddFL{" }& \DIFaddFL{Straight double quote }\\
\DIFaddFL{21. }& \DIFaddFL{RBR }& \DIFaddFL{Adverb, comparative }& \DIFaddFL{45. }& \DIFaddFL{' }& \DIFaddFL{Left open single quote }\\
\DIFaddFL{22. }& \DIFaddFL{RBS }& \DIFaddFL{Adverb, superlative }& \DIFaddFL{46. }& \DIFaddFL{" }& \DIFaddFL{Left open double quote }\\
\DIFaddFL{23. }& \DIFaddFL{RP }& \DIFaddFL{Particle }& \DIFaddFL{47. }& \DIFaddFL{' }& \DIFaddFL{Right close single quote }\\
\DIFaddFL{24. }& \DIFaddFL{SYM }& \DIFaddFL{Symbol (mathematical or scientific)}& \DIFaddFL{48. }& \DIFaddFL{" }& \DIFaddFL{Right close double quote

}

  \end{tabular}
  \label{table:penntagset}
\end{table}
\end{comment}


\DIFaddend \chapter{Transformaatiosäännöt}

\DIFdelbegin \DIFdel{TODO

}\DIFdelend \DIFaddbegin \DIFadd{Ensimmäiset automaattiset sanaluokkatunnistimet (mm. \mbox{%DIFAUXCMD
\citealp{greene1971}
}%DIFAUXCMD
) olivat lähtöisin kielitieteen piiristä, ja ne perustuivat pitkälti käsin laadittuihin kieliopillisiin transformaatiosääntöihin. Tällainen sääntöpohjainen menetelmä toimii seuraavasti: ensin kunkin sanan mahdolliset sanaluokat haetaan sanakirjasta tai vastaavasta tietolähteestä. Seuraavaksi niiden sanojen kohdalla, joiden sanaluokka ei ole yksikäsitteinen, sovelletaan valmiita transformaatiosääntöjä oikean sanaluokan tunnistamiseen. Transformaatiosäännöt voivat hyödyntää sekä lokaaleja että kontekstuaalisia kieliopillisia vihjeitä; tyypillisiä transformaatiosääntöjä ovat esimerkiksi }\textit{\DIFadd{korvaa substantiivi erisnimellä, jos sanalla on iso alkukirjain}} \DIFadd{(lokaali vihje) tai }\textit{\DIFadd{korvaa substantiivi verbillä, jos edeltävä sanaluokka on pronomini}} \DIFadd{(kontekstuaalinen vihje).

}\DIFaddend 

\DIFaddbegin \DIFadd{Tällaisen lähestymistavan ilmeisin heikkous on vaaditun manuaalisen työn määrä: erilaisille aineistoille tulee aina luoda uusi, aineiston kielelle ja tyylille spesifi sääntökokoelma. Huomattavan työpanoksen lisäksi sääntöpohjainen menetelmä vaatii myös ymmärryksen tulkittavan aineiston kieliopillisista ominaisuuksista, jotta luodut säännöt tuottavat toivotun tuloksen. Puhtaasti sääntöpohjaisilla menetelmillä voidaan saavuttaa --- sääntöjen määrästä riippuen --- korkeita tarkkuustuloksia, mutta kyseiset tulokset eivät ole siirrettävissä erilaisille aineistoille ilman mittavia muutoksia.

}

\DIFadd{Sääntöpohjaisten menetelmien puutteiden vuoksi useimmat nykyiset sanaluokkien tunnistajat perustuvat tilastollisiin menetelmiin: sanaluokkia koskeva tilastollinen informaatio voidaan poimia harjoitusaineistosta automaattisesti, siinä missä sääntöjen laatiminen vaatii lingvististä asiantuntemusta ja manuaalista työtä. Sääntöpohjaisilla menetelmillä on kuitenkin joitakin etuja tilastollisiin menetelmiin verrattuna: ensinnäkin kielioppisääntöjen tallentaminen vaatii huomattavasti vähemmän tallennustilaa kuin vastaava tilastollinen informaatio. Tilastollista informaatiota on myös hankalampi tulkita ja käsitellä kuin yksinkertaisia kielioppisääntöjä, jonka myötä tunnistusvirheiden tunnistaminen ja korjaaminen on helpompaa kielioppisääntöjä käytettäessä. \mbox{%DIFAUXCMD
\citep{brill1992}
}%DIFAUXCMD
}

\DIFadd{\mbox{%DIFAUXCMD
\citet{brill1992}
}%DIFAUXCMD
huomauttaakin, että tilastolliset tunnistajat saavuttavat korkean tunnistamistarkkuuden kiinnittämättä varsinaisesti huomiota aineiston taustalla olevaan kieliopilliseen rakenteeseen. Hän laskee tilastollisten menetelmien puutteeksi sen, että ne poimivat aineistosta lingvistisen informaation sijaan vain suuren määrän vaikeselkoisia tilastoja. 

}

\section{\DIFadd{Brillin sääntöpohjainen sanaluokkatunnistin}}

\DIFadd{\mbox{%DIFAUXCMD
\citet{brill1992,brill1994}
}%DIFAUXCMD
esittää artikkeleissaan vaihtoehtoisen lähestymistavan, joka pohjautuu varhaisimpien tunnistusmenetelmien tavoin kieliopillisiin sääntöihin. Aikaisemmista sääntöpohjaisista tunnistamismenetelmistä poiketen sääntöjä ei kuitenkaan syötetä manuaalisesti, vaan tunnistin oppii ne automaattisesti oikeilla sanaluokilla merkitystä harjoitusaineistosta. Menetelmän kantava idea on tunnistaa tehdyt tunnistusvirheet, ja inkrementaalisesti soveltaa virheitä korjaavia transformaatiosääntöjä kunnes ne eivät enään paranna kokonaistarkkuutta.

}

\subsection{\DIFadd{Alustus}}

\subsection{\DIFadd{Transformaatiosääntöjen soveltaminen}}

\section{\DIFadd{Arviota}}

\DIFaddend \chapter{Markovin piilomallit}

\DIFdelbegin \DIFdel{TODO
- esimerkkikuva Markovin ketjusta
- markov-oletus
- markovin ketjut
- bigrammit/trigrammit
- miksi piilotettu?

}\DIFdelend \DIFaddbegin \DIFadd{Edellisestä sääntöpohjaisesta menetelmästä poiketen useimmat nykyiset sanaluokkien tunnistajat perustuvat tilastollisiin menetelmiin. Tilastollisissa menetelmissä sanaluokkien tunnistaminen mielletään lingvistisen lähestymistavan sijaan koneoppimisongelmaksi, tai tarkemmin sarjanluokitteluongelmaksi. Sarjanluokitteluongelmassa tavoitteena on oppia funktio $f: \mathcal{X} \to \mathcal{Y}$, joka luokittelee kunkin syötteen $x$ johonkin luokkaan $y$. Todennäköisyyslaskennan kautta funktio $f$ voidaan määritellä muodossa

}\DIFaddend 

\DIFaddbegin \begin{align}\DIFadd{
f(x) = \argmax{y \in \mathcal{Y}} P(y|x)
}\end{align}

\DIFadd{missä todennäköisyyttä $P(y|x)$ estimoidaan harjoitusaineiston perusteella.

}

\DIFadd{Tilastolliset mallit voidaan vuorostaan jakaa diskriminatiivisiin ja generatiivisiin malleihin. Diskriminatiiviset mallit (esim. log-lineaariset mallit) käsittelevät suoraan todennäköisyyttä $P(y|x)$, eli ne eivät ota kantaa syötteeseen $x$. Generatiiviset mallit (esim. Markovin piilomallit) puolestaan käsittelevät koko yhteisjakaumaa $P(x,y)$, josta todennäköisyys $P(y|x)$ johdetaan Bayesin säännön avulla. Diskriminatiiviset mallit ovat siten generatiivisia malleja rajoittuneempia, mutta sarjanluokitteluongelman kannalta mallin rajoitteilla ei ole väliä. Intuition mukaan generatiivinen malli on diskriminatiivista mallia tehottomampi, sillä yhteisjakauman mallintaminen on laskennallisesti vaativampaa kuin ehdollisen jakauman. \mbox{%DIFAUXCMD
\citet{ng2002}
}%DIFAUXCMD
kuitenkin osoittavat, ettei tämä välttämättä aina pidä paikkaansa: sanaluokkien tunnistamiseen onkin olemassa kumpaankin malliin perustuvia tehokkaita ratkaisuja.

}

\section{\DIFadd{Markovin malli}}

\DIFaddend Markovin malli (mm. \citealp{rabiner1989}) kuvaa sellaista stokastista prosessia, \DIFdelbegin \DIFdel{jonka vallitseva }\DIFdelend \DIFaddbegin \DIFadd{joka toteuttaa ns. Markov-ominaisuuden: seuraava }\DIFaddend tila riippuu aina vain \DIFaddbegin \DIFadd{$N$:stä }\DIFaddend edeltä\DIFdelbegin \DIFdel{vistä}\DIFdelend \DIFaddbegin \DIFadd{vä}\DIFaddend ä\DIFdelbegin \DIFdel{tiloista. Yksinkertaisimmillaan malli koostuu havaintoja kuvaavista tiloista, joille kullekin on }\DIFdelend \DIFaddbegin \DIFadd{stä tilasta. Markov-ominaisuus on siis eräänlainen riippumattomuusoletus, joka yksinkertaistaa stokastisen prosessin tilan estimointia rajoittamalla tilasiirtymien historian }\DIFaddend määrä\DIFdelbegin \DIFdel{tty tilasiirtymien }\DIFdelend \DIFaddbegin \DIFadd{ä. $N$:nen asteen Markov-ominaisuuden toimiessa esimerkiksi }\DIFaddend todennäkö\DIFdelbegin \DIFdel{isyydet}\DIFdelend \DIFaddbegin \DIFadd{isyys

}

\begin{align}\DIFadd{
P(x_k | x_1, \ldots, x_{k-1})
}\end{align}

\DIFadd{voidaan laskea huomattavasti yksinkertaisemmin tarkastelemalla vain $N$:ää edellistä tilaa:

}

\begin{align}\DIFadd{
P(x_k | x_{k - N }, \ldots, x_{k-1})
}\end{align}

\DIFadd{Markov-ominaisuudesta puhuttaessa on yleensä kyse juuri ensimmäisen asteen Markov-ominaisuudesta, jolloin $N=1$}\DIFaddend . \DIFdelbegin \DIFdel{Usein prosessin tila ei kuitenkaan ole suoraan havaittavissa, eli tila on piilotettu, joskin havainnosta riippuvainen ; }\DIFdelend \DIFaddbegin \DIFadd{Käytännössä }\DIFaddend tä\DIFdelbegin \DIFdel{llä}\DIFdelend \DIFaddbegin \DIFadd{mä tarkoittaa sitä, että tarkastellaan jotakin kahta peräkkäistä tilaa --- nykyistä sekä tulevaa. Markov-ominaisuutta voidaan kuitenkin laajentaa myä}\DIFaddend ä\DIFdelbegin \DIFdel{in on kyse Markovin piilomallista. Sanaluokkien tunnistamisongelma voidaan kuvata kyseisenä}\DIFdelend \DIFaddbegin \DIFadd{s korkeampiin asteisiin, jolloin myös tarkasteltavien tilasarjojen pituudet kasvavat. Sanaluokkia tunnistaessa nä}\DIFaddend ä\DIFdelbegin \DIFdel{piilomallina: }\DIFdelend \DIFaddbegin \DIFadd{itä tilasarjoja vastaavat n:n peräkkäisen sanaluokan sarjat, eli ns. n-grammit.   

}

\DIFadd{TODO: Asteen valinnan merkitys.

}

\DIFadd{Yksinkertaisimmillaan Markovin mallia voidaan kuvata tilakoneena, joka koostuu havaittavia tapahtumia kuvaavista tiloista, sekä tilasiirtymämatriisista, josta ilmenevät todennäköisyydet siirtyä kustakin tilasta mihin tahansa muuhun tilaan. Kukin tila on siis itsenäinen ja muistiton.

}

\DIFadd{TODO Mahd. selitykset HMM:n viidestä elementistä \mbox{%DIFAUXCMD
\citep{rabiner1989}
}%DIFAUXCMD
, esimerkkikuva Markovin ketjusta.

}


\subsection{\DIFadd{Markovin piilomalli}}

\DIFadd{Markovin mallin hyödyllisyyttä rajoittaa se, että malli ei pysty itsessään mallintamaan prosesseja, joiden tilat eivät ole suoraan havaittavissa. Useimmissa mielenkiintoisissa tapauksissa prosessin tilat eivät kuitenkaan suoraan vastaa havaintoja, eli prosessin tila --- vaikkakin havainnosta riippuvainen --- on piilotettu: esimerkiksi sanaluokkien tunnistamisongelmassa }\DIFaddend havainto (sana) on tiedossa, tila (sanaluokka) on riippuvainen havainnosta, mutta tila itsessään ei ole tiedossa. \DIFaddbegin \DIFadd{Tällöin Markovin malli tulee laajentaa Markovin piilomalliksi, jossa havainto on aina sitä vastaavan tilan todennäköisyysfunktio.

}\DIFaddend 

\DIFaddbegin \DIFadd{TODO: tähän kuva Markovin piilomallista

}


\DIFaddend \section{Lähtökohta}

Tunnistamisongelmassa siis haetaan annetulla lauseelle sitä todennäköisimmin vastaavaa sanaluokkien sarjaa. Todennäköisyyttä voidaan mallintaa \DIFdelbegin \DIFdel{funktiolla

}\DIFdelend \DIFaddbegin \DIFadd{yhteisjakaumalla

}\DIFaddend 

\DIFdelbegin \[\DIFdel{
p(w_1, w_2, \ldots, w_n, t_1, t_2, \ldots, t_n)
}\]

%DIFAUXCMD
\DIFdelend \DIFaddbegin \begin{align}\DIFadd{
p(w_1, w_2, \ldots, w_n,\: t_1, t_2, \ldots, t_n)
\label{generativemodel}
}\end{align}

\DIFaddend 

mikä ilmaisee todennäköisyyden sille, että jokin lause $ w_1 \ldots w_n $ esiintyy jonkin sanaluokkasarjan $ t_1 \ldots t_n $ yhteydessä. Tällöin ratkaistavaksi jää

\DIFdelbegin \[\DIFdel{
\argmax{t_1 \ldots t_n} p(w_1, w_2, \ldots, w_n, t_1, t_2, \ldots, t_n)
}\]

%DIFAUXCMD
\DIFdelend \DIFaddbegin \begin{align}\DIFadd{
\argmax{t_1 \ldots t_n} p(w_1, w_2, \ldots, w_n,\: t_1, t_2, \ldots, t_n)
}\end{align}

\DIFaddend 

eli sanaluokkasarja $ t_1 \ldots t_n $, jolla saadaan maksimiarvo edeltävästä funktiosta. Mahdollisten sanaluokkasarjojen määrä kuitenkin kasvaa eksponentiaalisesti sanojen ja sanaluokkien määrän mukaan, jolloin maksimiarvon ratkaiseminen \DIFaddbegin \DIFadd{naiivisti }\DIFaddend on epätehokasta.


\section{Tunnistusongelma Markovin piilomallina}

Markovin \DIFdelbegin \DIFdel{piilomallien avulla }\DIFdelend \DIFaddbegin \DIFadd{piilomallit hyödyntävät sitä seikkaa, että yhteisjakauma $P(x,y)$ voidaan jakaa osiin $P(y)P(x|y)$. Tällöin }\DIFaddend edellämainittu funktio $ p $ \DIFaddbegin \eqref{generativemodel} \DIFaddend voidaan kuvata muodossa

\DIFdelbegin \[\DIFdel{
p(w_1, w_2, \ldots, w_n, t_1, t_2, \ldots, t_n) = \underbrace{\prod_{i=1}^{n+1} q(t_i | t_{i-2}, t_{i-1})}_\text{Markovin ketju}\prod_{i=1}^{n}e(w_i | t_i)
}\]

%DIFAUXCMD
\DIFdelend \DIFaddbegin \begin{align}\DIFadd{
& \hphantom{= } p(w_1, w_2, \ldots, w_n,\: t_1, t_2, \ldots, t_n) }\\
& \DIFadd{= p(t_1, t_2, \ldots, t_n)\:p(w_1, w_2, \ldots, w_n|t_1, t_2, \ldots, t_n) }\\
& \DIFadd{= \prod_{i=1}^{n+1} q(t_i | t_{i-2}, t_{i-1}) \prod_{i=1}^{n}e(w_i | t_i)
}\end{align}

\DIFaddend 

missä $t_0$ ja $t_{-1}$ ovat lauseen alkuun lisättyjä alkusanaluokkia, ja $t_{n+1}$ on päätemerkkisanaluokka. Mallin ensimmäinen parametri

\DIFdelbegin \[\DIFdel{
q(t_i | t_{i-2}, t_{i-1})
}\]

%DIFAUXCMD
\DIFdelend \DIFaddbegin \begin{align}\DIFadd{
q(t_i | t_{i-2}, t_{i-1})
}\end{align}

\DIFaddend 

laskee todennäköisyyden sanaluokalle $t_i$, kun kaksi edeltävää sanaluokkaa ovat $t_{i-1}$ ja $t_{i-2}$. \DIFdelbegin \DIFdel{Tä}\DIFdelend \DIFaddbegin \DIFadd{Parametri voidaan myös mieltä}\DIFaddend ä\DIFdelbegin \DIFdel{stä}\DIFdelend \DIFaddbegin \DIFadd{ä}\DIFaddend ä \DIFdelbegin \DIFdel{trigrammista voidaan pä}\DIFdelend \DIFaddbegin \DIFadd{todennä}\DIFaddend ä\DIFdelbegin \DIFdel{ä}\DIFdelend \DIFaddbegin \DIFadd{köisyytenä}\DIFaddend ä \DIFdelbegin \DIFdel{tellä}\DIFdelend \DIFaddbegin \DIFadd{trigrammille $t_{i-2}, t_{i-1}, t_i $. Tä}\DIFaddend ä\DIFaddbegin \DIFadd{stä huomataan}\DIFaddend , että kyseessä on \DIFdelbegin \DIFdel{ns. }\DIFdelend toisen asteen Markovin piilomalli. Mallin toinen parametri

\DIFdelbegin \[\DIFdel{
e(w_i | t_1)
}\]

%DIFAUXCMD
\DIFdelend \DIFaddbegin \begin{align}\DIFadd{
e(w_i | t_i)
}\end{align}

\DIFaddend 

laskee todennäköisyyden sille, että sana $w_i$ esiintyy sanaluokan $t_i$ yhteydessä.


\subsection{Parametrien estimointi}

Yksinkertaisimmillaan parametri $q(t_i|t_{i-2},t_{i-1})$ voidaan estimoida laskemalla

\DIFdelbegin \[\DIFdel{
q(t_3|t_1,t_2) = \frac{f(t_1,t_2,t_3)}{f(t_1,t_2)}
}\]

%DIFAUXCMD
\DIFdelend \DIFaddbegin \begin{align}\DIFadd{
q(t_i|t_{i-2},t_{i-1}) = \frac{f(t_{i-2},t_{i-1},t_i)}{f(t_{i-1},t_{i-1})}
}\end{align}

\DIFaddend 

missä funktio $f$ merkitsee annetun n-grammin lukumäärää harjoitusaineistossa. \citet{brants2000} kuitenkin osoittaa, \DIFdelbegin \DIFdel{ettei }\DIFdelend \DIFaddbegin \DIFadd{että datan harvuuden vuoksi }\DIFaddend tällainen estimaatti \DIFaddbegin \DIFadd{ei }\DIFaddend ole käyttökelpoinen\DIFdelbegin \DIFdel{, sillä }\DIFdelend \DIFaddbegin \DIFadd{: }\DIFaddend laajassakaan harjoitusaineistossa ei ole tarpeeksi montaa kappaletta kutakin eri trigrammia. Lisäksi osa trigrammeista \DIFdelbegin \DIFdel{$t_i,t_{i+1},t_{i+2}$ }\DIFdelend \DIFaddbegin \DIFadd{$t_{i-2},t_{i-1},t_i$ }\DIFaddend ovat väistämättä sellaisia, että \DIFdelbegin \DIFdel{$f(t_i,t_{i+1},t_{i+2}) = 0$}\DIFdelend \DIFaddbegin \DIFadd{$f(t_{i-2},t_{i-1},t_i) = 0$}\DIFaddend , jolloin koko sarja $t_1 \ldots t_n$ saa todennäköisyyden $0$. Luotettavampi tapa estimoida arvoa $q$ on hyödyntää trigrammien lisäksi myös harjoitusaineistosta johdettujen uni- ja \DIFdelbegin \DIFdel{digrammien }\DIFdelend \DIFaddbegin \DIFadd{bigrammien }\DIFaddend suhteellisia frekvenssejä:

\DIFdelbegin \begin{align*}\DIFdel{
Unigrammi}&\DIFdel{: P(t_3) = \frac{f(t_3)}{N} }\\
\DIFdel{Digrammi}&\DIFdel{: P(t_3 | t_2) = \frac{f(t_2, t_3)}{f(t_2)} }\\
\DIFdel{Trigrammi}&\DIFdel{: P(t_3 | t_1, t_2) = \frac{f(t_1,t_2,t_3)}{f(t_1,t_2)}
}\end{align*}

%DIFAUXCMD
\DIFdelend \DIFaddbegin \begin{align}\DIFadd{
Unigrammi}&\DIFadd{: P(t_i) = \frac{f(t_i)}{N} }\\
\DIFadd{Bigrammi}&\DIFadd{: P(t_i | t_{i-1}) = \frac{f(t_{i-1}, t_i)}{f(t_{i-1})} }\\
\DIFadd{Trigrammi}&\DIFadd{: P(t_i | t_{i-2}, t_{i-1}) = \frac{f(t_{i-2},t_{i-1},t_i)}{f(t_{i-2},t_{i-1})}
}\end{align}

\DIFaddend 

missä $N$ merkitsee harjoitusaineiston sanojen kokonaislukumäärää. Nyt funktion $q$ arvoa voidaan silottaa interpoloimalla edellämainittuja n-grammeja:

\DIFdelbegin \[\DIFdel{
q(t_3 | t_1, t_2) = \lambda_1 P(t_3) + \lambda_2 P(t_3 | t_2) + \lambda_3 P(t_3 | t_1, t_2)
}\]

%DIFAUXCMD
\DIFdelend \DIFaddbegin \begin{align}\DIFadd{
q(t_i | t_{i-2}, t_{i-1}) = \lambda_1 P(t_i) + \lambda_2 P(t_i | t_{i-1}) + \lambda_3 P(t_i | t_{i-2}, t_{i-1})
}\end{align}

\DIFaddend 

missä $\lambda_1+\lambda_2+\lambda_3 = 1$ ja $\lambda_1,\lambda_2,\lambda_3 \geq 0$ (TODO muut silotusmenetelmät, perustelu interpolaatiolle \DIFaddbegin \DIFadd{ja lambda-arvoille}\DIFaddend ). Vastaavasti todennäköisyys $e$ voidaan estimoida vertaamalla sana-sanaluokka-yhdistelmän frekvenssiä pelkän sanaluokan frekvenssiin:

\DIFdelbegin \[\DIFdel{
e(w_3|t_3) = \frac{f(w_3, t_3)}{f(t_3)}
}\]

%DIFAUXCMD
\DIFdelend \DIFaddbegin \begin{align}\DIFadd{
e(w_i|t_i) = \frac{f(w_i, t_i)}{f(t_i)}
\label{hmm_param_e}
}\end{align}

\DIFaddend 

\subsection{Tuntemattomien sanojen käsittely}

\DIFdelbegin \DIFdel{Edellinen todennä}\DIFdelend \DIFaddbegin \DIFadd{Todennä}\DIFaddend äköisyyden $e$ estimaatti \DIFaddbegin \eqref{hmm_param_e} \DIFaddend ei kuitenkaan ole luotettava, jos \DIFdelbegin \DIFdel{sana $w$ }\DIFdelend \DIFaddbegin \DIFadd{jokin kohdattu sana }\DIFaddend ei esiinny harjoitusaineistossa kertaakaan. Tällöin\DIFdelbegin \DIFdel{$e(w|t)=0$ }\DIFdelend \DIFaddbegin \DIFadd{, jos sana $w_i$ on tuntematon, on $e(w_i|t_i)=0$ }\DIFaddend millä tahansa sanaluokalla \DIFdelbegin \DIFdel{$t$, ja samoin 
}\DIFdelend \DIFaddbegin \DIFadd{$t_i$. Samoin }\DIFaddend $p(w_1, w_2, \ldots, w_n, t_1, t_2, \ldots, t_n) = 0$, jos yksikään sanoista \DIFdelbegin \DIFdel{$w_1 \ldots w_n$ }\DIFdelend \DIFaddbegin \DIFadd{$w_i \ldots w_n$ }\DIFaddend on tuntematon. \DIFaddbegin \DIFadd{Jotta vältytään mallin rikkovilta nollaestimaateilta, on tuntemattomien sanojen kohdalla sovellettava jonkinlaista poikkeuskäsittelyä.

}

\DIFaddend Yksinkertaisin ratkaisu on määrätä tuntemattoman sanalle aina harjoitusaineiston yleisin sanaluokka, \DIFdelbegin \DIFdel{kä}\DIFdelend \DIFaddbegin \DIFadd{yleensä}\DIFaddend ä \DIFdelbegin \DIFdel{ytännössä }\DIFdelend substantiivi. Joitain kieliä --- kuten englantia --- tulkittaessa voidaan saavuttaa parempia tuloksia suffiksianalyysin \citep{samuelsson1993} avulla. Tällöin tarkoituksena on hyödyntää sitä seikkaa, että sanan pääte on usein vahva indikaattori sen sanaluokasta. Lisäksi \citet{toutanova2003} ovat esittäneet, kuinka seuraavassa kappaleessa esiteltäviä log-lineaarisia malleja voidaan hyödyntää tuntemattomien sanojen käsittelyssä.

\DIFaddbegin \DIFadd{\mbox{%DIFAUXCMD
\citet{bikel1999}
}%DIFAUXCMD
esittivät vaihtoehtoisen, pseudosanoihin perustuvan menetelmän tuntemattomien sanojen käsittelylle. Menetelmän perusajatuksena on korvata tunnistettavan aineiston kukin tuntematon sana jollakin pseudosanalla, joita on rajallinen määrä. Myös kaikki harvinaiset (vähemmän kuin 5 esiintymää) sanat voidaan korvata pseudosanoilla. Korvaava pseudosana määräytyy aina tuntemattoman sanan ominaisuuksien mukaan: esimerkiksi }\textit{\DIFadd{isoAlkukirjain}}\DIFadd{, }\textit{\DIFadd{lauseenEnsimmäinenSana}} \DIFadd{sekä }\textit{\DIFadd{neljäNumeroa}} \DIFadd{ovat tyypillisiä pseudosanoja. Nyt kun harjoitusaineiston harvinaiset sanat korvataan vastaavilla pseudosanoilla, voidaan tunnistettavan aineiston pseudosanoja käsitellä samoin kuin tavallisia sanoja.

}


\DIFaddend \subsection{Viterbin algoritmi}

Tässä kappaleessa kuvaillaan lyhyesti Viterbin algoritmia, jolla ratkaistaan tehokkaasti em. arvo $\argmax{t_1 \ldots t_n} p(w_1, w_2, \ldots, w_n, t_1, t_2, \ldots, t_n)$.

\section{Parannukset (tjsp)}

Tähän kuvaukset Cyclic Dependency Network-menetelmästä \citep{toutanova2003}, mahd. ohjaamattomasta oppimisesta \citep{banko2004}. 

\DIFdelbegin \DIFdel{The trigram assumption is arguably quite strong, and linguistically naive. However, it leads to models that are very useful in
practice.

}\DIFdelend 


\chapter{Log-lineaariset mallit}

TODO


\DIFdelbegin \chapter{\DIFdel{Menetelmien vertailua/analyysia}}

%DIFAUXCMD
\addtocounter{chapter}{-1}%DIFAUXCMD
%DIFDELCMD < 

%DIFDELCMD < 
%DIFDELCMD < %%%
\DIFdel{TODO Tässä kappaleessa esitellään tunnistimien eri toimintaympäristöt (esim. erilaiset kielet, aineistot, harjoitusaineiston saatavuus), ja pohditaan miten eri menetelmät toimivat eri olosuhteissa; vastataan siis tutkimuskysymykseen (miten eri menetelmät eroavat toisistaan, ja soveltuvatko jotkin menetelmät toisia paremmin tietyille toimintaympäristöille).

}%DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdel{Toisaalta voisi olla mielekkäämpää perustella menetelmien etuja jo aikaisemmissa kappaleissa, niiden esittelyiden yhteydessä. Menetelmien esitysjärjestys on sellainen, että myöhempi menetelmä tavallaan vastaa aikaisemman menetelmän puutteisiin.  Lisäksi tässä kappaleessa esiteltäviä johtopäätöksiä voisi jättää yhteenveto-kappaleeseen; kokonaisen kappaleen verran johtopäätöksiä ja merkityksellistä analyysia vaikuttaa turhan kunnianhimoiselta tavoitteelta.  Harkitsen siis tämän kappaleen poistamista.

}%DIFDELCMD < 

%DIFDELCMD < 
%DIFDELCMD < %%%
\DIFdelend \chapter{Yhteenveto}

TODO Yhteenvedossa kerrataan työn pääkohdat lyhyehkösti johtopäätöksiä tehden. Siinä voi myös esittää pohdintoja siitä, minkälaisia tutkimuksia aiheesta voisi jatkossa tehdä. \DIFaddbegin \DIFadd{viittaa kirjallisuuskatsauksen tarkoitukseen ja kertoo ''päätulokset''.
TODO vastataan tutkimuskysymykseen

}\DIFaddend 




\begin{thebibliography}{}

\bibitem[Banko \& Moore(2004)]{banko2004}
Banko, M. \& Moore, R. C. \DIFdelbegin \DIFdel{(2004). }\DIFdelend \DIFaddbegin \DIFadd{2004. }\DIFaddend \textit{Part of speech tagging in context}. Proceedings of the 20th conference on Computational Linguistics, ACL, s. 556.

\DIFaddbegin \bibitem[Bikel ym.(1999)]{bikel1999}
\DIFadd{Bikel, D. M., Schwartz, R., }\& \DIFadd{Weischedel, R. M. 1999. }\textit{\DIFadd{An algorithm that learns what's in a name}}\DIFadd{. Machine learning, 34(1-3), s.~211--231.

}

\DIFaddend \bibitem[Brants(2000)]{brants2000}
Brants, T. 2000. \textit{TnT - A statistical part-of-speech tagger}. Proceedings of the 6th Applied NLP Conference (ANLP).

\bibitem[Brill(1992)]{brill1992}
Brill, E. 1992. \textit{A simple rule-based part of speech tagger}. Proceedings of the 3rd conference on Applied Computational Linguistics, ACL, Trento, Italy.

\DIFdelbegin %DIFDELCMD < \bibitem[Brill(1995)]{brill1995}
%DIFDELCMD < %%%
\DIFdelend \DIFaddbegin \bibitem[Brill(1994)]{brill1994}
\DIFaddend Brill, E. \DIFdelbegin \DIFdel{1995. }\DIFdelend \DIFaddbegin \DIFadd{1994. }\DIFaddend \textit{\DIFdelbegin \DIFdel{Transformation-based error-driven learning and natural language processing: a case study }\DIFdelend \DIFaddbegin \DIFadd{Some advances }\DIFaddend in \DIFdelbegin \DIFdel{part-of-speech }\DIFdelend \DIFaddbegin \DIFadd{transformation-based part of speech }\DIFaddend tagging}. \DIFdelbegin \DIFdel{Computational Linguistics, 21(4), }\DIFdelend \DIFaddbegin \DIFadd{AAAI 1994, }\DIFaddend s.~\DIFdelbegin \DIFdel{543-565}\DIFdelend \DIFaddbegin \DIFadd{722-727}\DIFaddend .

\bibitem[Charniak ym.(1993)]{charniak1993}
Charniak, E., Hendrickson, C., Jacobson, N., \& Perkowitz, M. 1993. \textit{Equations for part-of-speech tagging}. Proceedings of AAAI-93, s.~784--789.

\bibitem[Church(1988)]{church1988}
Church, K. 1988. \textit{A stochastic parts program and noun phrase parser for unrestricted text}. Proceedings of the 2nd conference on Applied Natural Language Processing, s.~136--143.

\bibitem[Cutting ym.(1992)]{cutting1992}
Cutting, D., Kupiec, J., Pedersen, J. \& Sibun, P. 1992. \textit{A Practical Part-of-Speech Tagger}. Proceedings of the 3rd conference on Applied Natural Language Processing, s.~133--140.

\bibitem[DeRose(1988)]{derose1988}
DeRose, S. J. 1988. \textit{Grammatical category disambiguation by statistical optimization}. Computational Linguistics, 14(1), s.~31--39.

\DIFdelbegin %DIFDELCMD < \bibitem[Francis(1964)]{francis1964}
%DIFDELCMD < %%%
\DIFdel{Francis, W. N. 1964. }\textit{\DIFdel{A standard sample of present-day English for the use with digital computers}}%DIFAUXCMD
\DIFdel{. Report for the U.S Office of Education on Cooperative Research Project No. E-007.
Brown University, Providence RI.

}%DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdelend \bibitem[Garside(1987)]{garside1987}
Garside, R. 1987. \textit{The CLAWS word-tagging system}. Teoksessa R. Garside, G. Leech \& G. Sampson (toim.) The Computational Analysis of English: A Corpus-based Approach. London: Logman, s.~30-41.

\DIFaddbegin \bibitem[Greene \& Rubin(1971)]{greene1971}
\DIFadd{Greene, B. B., }\& \DIFadd{Rubin, G. M. 1971. }\textit{\DIFadd{Automatic grammatical tagging of English}}\DIFadd{. Department of Linguistics, Brown University, 1971.

}

\DIFaddend \bibitem[Heckerman ym.(2000)]{heckerman2000}
Heckerman, D., Chickering, D. M., Meek, C., Rounthwaite, R. \& Kadie, C. M. 2000. \textit{Dependency networks for inference, collaborative filtering and data visualization}.
Journal of Machine Learning Research, 1(1), s.~49-75.

\bibitem[Jaynes(1957)]{jaynes1957}
Jaynes, E. T. 1957. \textit{Information Theory and Statistical Mechanics}. Physical Review, 106, s.~620-630.

\bibitem[Manning(2011)]{manning2011}
Manning, C. D. 2011. \textit{Part-of-Speech Tagging from 97\% to 100\%: Is It Time for Some Linguistics?} Proceedings of the 12th international conference on Computational linguistics and intelligent text processing, 1, s.~171--189.

\DIFdelbegin %DIFDELCMD < \bibitem[Manning \& Schütze(1999)]{manning1999}
%DIFDELCMD < %%%
\DIFdelend \DIFaddbegin \bibitem[Manning \& Sch\"{u}tze(1999)]{manning1999}
\DIFaddend Manning, C. D. \& \DIFdelbegin \DIFdel{Schä}\DIFdelend \DIFaddbegin \DIFadd{Sch}\"{u}\DIFaddend tze, H. 1999. \textit{Foundations of statistical natural language processing}.
Cambridge, MA\DIFdelbegin \DIFdel{: }\DIFdelend \DIFaddbegin \DIFadd{. }\DIFaddend MIT Press

\bibitem[Marcus ym.(1993)]{marcus1993}
Marcus, M. P., Marcinkiewicz, M. \& Santorini, B. 1993. \textit{Building a large annotated corpus of English: the Penn treebank}. Computational Linguistics, 19(2), s.~313-330.

\bibitem[Merialdo(1994)]{merialdo1994}
Merialdo, B. 1994. \textit{Tagging English text with a probabilistic model}. Computational Linguistics, 20(2), s.~155-171.

\DIFdelbegin %DIFDELCMD < \bibitem[Petrov ym.(2011)]{petrov2011}
%DIFDELCMD < %%%
\DIFdel{Petrov, S. , Das, D}\DIFdelend \DIFaddbegin \bibitem[Ng \& Jordan(2002)]{ng2002}
\DIFadd{Ng, A. Y}\DIFaddend . \& \DIFdelbegin \DIFdel{McDonald, R. 2011.
}\DIFdelend \DIFaddbegin \DIFadd{Jordan, M. 2002. }\DIFaddend \textit{\DIFaddbegin \DIFadd{On Discriminative vs. Generative Classifiers: }\DIFaddend A \DIFdelbegin \DIFdel{universal part-of-speech tagset}\DIFdelend \DIFaddbegin \DIFadd{comparison of logistic regression and Naive Bayes}\DIFaddend }. \DIFdelbegin \DIFdel{ArXiv:1104.2086.

}\DIFdelend \DIFaddbegin \DIFadd{In NIPS 14.

}\DIFaddend 

\bibitem[''POS Tagging State of the Art''(2013)]{aclwiki2013}
\textit{POS Tagging State of the Art}. 2013. The Wiki of the Association for Computational Linguistics.
Haettu 28.10.2013, osoitteesta \url{aclweb.org/aclwiki/index.php?title=POS_Tagging_(State_of_the_art)}

\bibitem[Rabiner(1989)]{rabiner1989}
Rabiner, L. R. 1989. \textit{A tutorial on Hidden Markov Models and selected applications in speech recognition}. Proceedings of the IEEE, 77(2), s.~257-285.

\bibitem[Ratnaparkhi(1996)]{ratnaparkhi1996}
Ratnaparkhi, A. 1996. \textit{A maximum entropy model for part-of-speech tagging}. Proceedings of the Empirical Methods in Natural Language Processing Conference (EMNLP), University of Pennsylvania.

\bibitem[Ratnaparkhi(1997)]{ratnaparkhi1997}
Ratnaparkhi, A. 1997. \textit{ A simple introduction to maximum entropy models for natural language processing}.
Technical Report 97-08, Institute for Research in Cognitive Science, University of Pennsylvania, 1997.

\bibitem[Samuelsson(1993)]{samuelsson1993}
Samuelsson, C. 1993. \textit{Morphological tagging based entirely on Bayesian inference}. Proceedings of the 9th Nordic Conference on Computational Linguistics NODALIDA-93.

\bibitem[Shen ym.(2007)]{shen2007}
Shen, L., Satta, G. \& Joshi, A. 2007. \textit{Guided learning for bidirectional sequence classification}. In: ACL 2007. (2007)

\bibitem[Spoustova ym.(2009)]{spoustova2009}
Spoustova, D.j., Hajic, J., Raab, J. \& Spousta, M. 2009. \textit{Semi-supervised training for the averaged perceptron POS tagger}. Proceedings of the 12th Conference of the
European Chapter of the ACL (EACL 2009), s.~763-711\DIFdelbegin \DIFdel{.

}%DIFDELCMD < 

%DIFDELCMD < \bibitem[Søgaard(2010)]{sogaard2010}
%DIFDELCMD < %%%
\DIFdel{Søgaard, A. 2010. }\textit{\DIFdel{Simple semi-supervised training of part-of-speech taggers}}%DIFAUXCMD
\DIFdel{. Proceedings of the ACL 2010 Conference Short Papers, s.~205-208}\DIFdelend .

\bibitem[Toutanova ym.(2003)]{toutanova2003}
Toutanova, K., Klein, D., Manning, C.D. \& Singer, Y. 2003. \textit{Feature-rich part-of-speech tagging with a cyclic dependency network}. In: NAACL 3. (2003), s.~252-259

\bibitem[Tseng ym.(2005)]{tseng2005}
Tseng, H., Jurafsky, D. \& Manning, C. 2005. \textit{Morphological features help POS tagging of unknown words across language varities}. Proceedings of the 4th SIGHAN bakeoff.

\end{thebibliography}

\end{document}
