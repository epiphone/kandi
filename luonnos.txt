#==========================================#
| Sanaluokkien automaattinen tunnistaminen |
#==========================================#

### OVERFITTING ### ylisovittaminen

Overfitting/Overtraining the model occurs when model parameters correctly
represent the training set but the model cannot generalize the training data to a
larger set.

Ramshaw & Marcus, 1994:
- One of the intriguing features of rule sequence learning is its apparent resistance to overtraining.

### BRILL ###

TODO
+ yksinkertainen
+ vaatii vähän muistia vrt. tilastolliset menetelmät
+ lopputuloksena saadaan lingvistisesti merkityksellistä ja helposti tulkittavaa dataa --- sääntöjä
- hitaampi opettaa
- huonompi tarkkuus (?)
plussat:
 - vast reduction in stored information required
 - perspicuity of small set of meaningful rules
 - ease of finding and implementing improvements
 - better portability from one tag set, corpus genre or language to another (?)
 - konseptuaalisesti yksinkertaisempi !!!
miinukset:
 - tuntemattomien sanojen heikko tarkkuus

http://www.inf.fu-berlin.de/lehre/WS02/sprachverarbeitung/Reader/14.Termin_Brill_Tagger/brill-slides.pdf
- can capture more context than Markov models
- always learns on the whole data set - no data sparseness:

### MARKOV ###

1. kyseessä on sarjanluokitteluongelma, ensin P(y|x)

2. generatiivinen malli P(x,y) eli yhteisjakauma

3. P(x,y) = p(y)p(x|y) (ongelma jaetaan)

TnT: overfit !!!

leksikaalinen e() ja kontekstuaalinen d() !!!

The trigram assumption is arguably quite strong, and linguistically naive. However, it leads to models that are very useful in practice.

n-grammit
+ voi poimia sellaisia rakenteita/vihjeitä joita ihminen ei huomaisi (vrt. sääntöpohjaiset menetelmät)
- "does not model long-distance relationships"
- yksinkertaistava oletus: Markovin oletus
  - vain edeltävillä on väliä
- n-gram-mallin heikkous:
  - "we can extend to trigrams, 4-grams, 5-grams; in general this is an insufficient model of languages,
    because language has long-distance dependencies:
    'The computer which I had just put into the machine room on the fifth floor crashed'"
    Sana 'crashed' pitäisi arvata sanan 'floor' perusteella, vaikka objekti on lauseen 2. sana!
  - But we can often get away with n-grams
  - Selecting the order of a Markov model:
    - higher order models remember more “history”
    - additional history can have predictive value
    - but the number of parameters we need to estimate  grows exponentially with the order
      – for modeling DNA we need parameters for an nth order model
      - the higher the order, the less reliable we can expect our parameter estimates to be
      – estimating the parameters of a 2nd order Markov chain from the complete genome of E. Coli, we’d
        see each word > 72,000 times on average
      – estimating the parameters of an 8th order chain, we’d see each word ~ 5 times on average

Giesbrecht & Evert (2009):
- HMM taggers are more robust and much faster than advanced machine-learning approaches such as MaxEnt !!!
- [...] we see that the best overall accuracy is now achieved by TnT, a HMM-based tagger. While the Stanford tagger is considerably better than its competitors on unknown words, its overall accuracy falls slightly short of TnT certain degree of overtraining for the machinelearning approaches (Stanford and SVM tagger), while TnT generalizes better to less standardized genres such as Web texts. We may thus conclude that HMM-based approaches are both more robust and computationally more efficient than MaxEnt and other advanced machine-learning techniques.

Brill 95:
"In the Markov model typically used for stochastic tagging, state
transition probabilities (P(Tagi I Tagi_l... Tagi-n)) express the likelihood of a tag immediately
following n other tags, and emit probabilities (P(Wordj I Tagi)) express the
likelihood of a word, given a tag. Many useful relationships, such as that between a
word and the previous word, or between a tag and the following word, are not directly
captured by Markov-model based taggers. The same is true of the nonlexicalized
transformation-based tagger, where transformation templates do not make reference
to words."


### YHTEENVETO ###

Tässä tutkielmassa käsiteltiin jalkapallon pelianalyysin ohjelmistoja.
Ohjelmistoista on hyötyä - -.
Nykyohjelmien rajoituksena voidaan pitää - -.
kertoo johtopäätöksiä tai antaa suosituksia
Tällä hetkellä pelianalyysialalla on useita kilpailevia ohjelmistoja, mutta yleinen, pelin eri osa-alueet huomioonottava ohjelma tuntuu puuttuvan.
Tulevaisuudessa - -. Tällä tavoin pelianalyyseillä
pystyttäisiin kehittämään jokaiselle joukkueelle paras
mahdollinen pelitapa. voi sisältää työn arviointia tai jatkotutkimushaasteita
