#==========================================#
| Sanaluokkien automaattinen tunnistaminen |
#==========================================#

### OVERFITTING ### ylisovittaminen

Overfitting/Overtraining the model occurs when model parameters correctly
represent the training set but the model cannot generalize the training data to a
larger set.

Ramshaw & Marcus, 1994:
- One of the intriguing features of rule sequence learning is its apparent resistance to overtraining.

### BRILL ###

TODO
+ yksinkertainen
+ vaatii vähän muistia vrt. tilastolliset menetelmät
+ lopputuloksena saadaan lingvistisesti merkityksellistä ja helposti tulkittavaa dataa --- sääntöjä
- hitaampi opettaa
- huonompi tarkkuus (?)
plussat:
 - vast reduction in stored information required
 - perspicuity of small set of meaningful rules
 - ease of finding and implementing improvements
 - better portability from one tag set, corpus genre or language to another (?)
 - konseptuaalisesti yksinkertaisempi !!!
miinukset:
 - tuntemattomien sanojen heikko tarkkuus

http://www.inf.fu-berlin.de/lehre/WS02/sprachverarbeitung/Reader/14.Termin_Brill_Tagger/brill-slides.pdf
- can capture more context than Markov models
- always learns on the whole data set - no data sparseness:

### MARKOV ###

1. kyseessä on sarjanluokitteluongelma, ensin P(y|x)

2. generatiivinen malli P(x,y) eli yhteisjakauma

3. P(x,y) = p(y)p(x|y) (ongelma jaetaan)

TnT: overfit !!!

leksikaalinen e() ja kontekstuaalinen d() !!!

The trigram assumption is arguably quite strong, and linguistically naive. However, it leads to models that are very useful in practice.

n-grammit
+ voi poimia sellaisia rakenteita/vihjeitä joita ihminen ei huomaisi (vrt. sääntöpohjaiset menetelmät)
- "does not model long-distance relationships"
- yksinkertaistava oletus: Markovin oletus
  - vain edeltävillä on väliä
- n-gram-mallin heikkous:
  - "we can extend to trigrams, 4-grams, 5-grams; in general this is an insufficient model of languages,
    because language has long-distance dependencies:
    'The computer which I had just put into the machine room on the fifth floor crashed'"
    Sana 'crashed' pitäisi arvata sanan 'floor' perusteella, vaikka objekti on lauseen 2. sana!
  - But we can often get away with n-grams
  - Selecting the order of a Markov model:
    - higher order models remember more “history”
    - additional history can have predictive value
    - but the number of parameters we need to estimate  grows exponentially with the order
      – for modeling DNA we need parameters for an nth order model
      - the higher the order, the less reliable we can expect our parameter estimates to be
      – estimating the parameters of a 2nd order Markov chain from the complete genome of E. Coli, we’d
        see each word > 72,000 times on average
      – estimating the parameters of an 8th order chain, we’d see each word ~ 5 times on average

Giesbrecht & Evert (2009):
- HMM taggers are more robust and much faster than advanced machine-learning approaches such as MaxEnt !!!
- [...] we see that the best overall accuracy is now achieved by TnT, a HMM-based tagger. While the Stanford tagger is considerably better than its competitors on unknown words, its overall accuracy falls slightly short of TnT certain degree of overtraining for the machinelearning approaches (Stanford and SVM tagger), while TnT generalizes better to less standardized genres such as Web texts. We may thus conclude that HMM-based approaches are both more robust and computationally more efficient than MaxEnt and other advanced machine-learning techniques.

Brill 95:
"In the Markov model typically used for stochastic tagging, state
transition probabilities (P(Tagi I Tagi_l... Tagi-n)) express the likelihood of a tag immediately
following n other tags, and emit probabilities (P(Wordj I Tagi)) express the
likelihood of a word, given a tag. Many useful relationships, such as that between a
word and the previous word, or between a tag and the following word, are not directly
captured by Markov-model based taggers. The same is true of the nonlexicalized
transformation-based tagger, where transformation templates do not make reference
to words."


### MAXENT ###

On Discriminative vs. Generative classifiers (Ng & Jordan) !!!
- generative classifiers learn a model of the joint probability p(x,y) of the inputs x and the label y,
  and make their predictions using Bayes rules to calculate p(y|x), and then picking the most likely label y.
- Discriminative classifiers model the posterior p(y|x) directly, or learn a direct map from inputs x to the
  class labels.
- "one should solve the classification problem directly and never solve more general problem as an
  intermediate step" Ng eri mieltä?
-

Wikipedia - Generative Model:
Generative models contrast with discriminative models, in that a generative model is a full probabilistic model of all variables, whereas a discriminative model provides a model only for the target variable(s) conditional on the observed variables. Thus a generative model can be used, for example, to simulate (i.e. generate) values of any variable in the model, whereas a discriminative model allows only sampling of the target variables conditional on the observed quantities.

- generatiivinen (yhteisjakauma, "generate the observed data from hidden stuff") vs diskriminatiivinen (ehdollinen jakauma)
- generatiiviset mallit
  - painot triviaaleja: just relative frequencies
- diskriminatiiviset mallit (https://class.coursera.org/nlp/lecture/38)
  - korkea tarkkuus
  - helposti laajennettavissa, virheitä tutkimalla, observation! rich features -> major improvements
  - " in that a generative model is a full probabilistic model of all variables, whereas a discriminative model provides a model only for the target variable(s) conditional on the observed variables. Thus a generative model can be used, for example, to simulate (i.e. generate) values of any variable in the model, whereas a discriminative model allows only sampling of the target variables conditional on the observed quantities." - WikiPedia
  - training is slower, requires linguistic understanding
  - they make it easy to incorporate lots of linguistically important features
  - Instead of putting probabilities over observed data (i.e. P(data,class) ) like joint models (e.g. Naive Bayes) do, conditional models take data as given and put probabilities over hidden structures (i.e. P(class|data) ) because these probabilities are more closely related to classification success. Conditional models seek to maximize these conditional probabilities.

- features
  - 1. an indicator function
  - 2. a particular class
- weights
- crucially two expectations:
  - empirical count (training data) of a feature
  - model expectation of a feature

Brill & Wu 1998:
The maximum-entropy framework is a
probabilistic framework where a model is found
that is consistent with the observed data and is
maximally agnostic with respect to all
parameters for which no data exists. It is a nice
framework for combining multiple constraints.
Whereas the transformation-based tagger
enforces multiple constraints by having multiple
rules fire, the maximum-entropy tagger can have
all of these constraints play a role at setting the
probability estimates for the model's parameters.
In Ratnaparkhi (1996), a maximum entropy
tagger is presented. The tagger uses essentially
the same parameters as the transformation-based
tagger, but employs them in a different model.
For our experiments, we used a publicly
available implementation of maximum-entropy
tagging) retrained on our training set.



### YHTEENVETO ###

Tässä tutkielmassa käsiteltiin jalkapallon pelianalyysin ohjelmistoja.
Ohjelmistoista on hyötyä - -.
Nykyohjelmien rajoituksena voidaan pitää - -.
kertoo johtopäätöksiä tai antaa suosituksia
Tällä hetkellä pelianalyysialalla on useita kilpailevia ohjelmistoja, mutta yleinen, pelin eri osa-alueet huomioonottava ohjelma tuntuu puuttuvan.
Tulevaisuudessa - -. Tällä tavoin pelianalyyseillä
pystyttäisiin kehittämään jokaiselle joukkueelle paras
mahdollinen pelitapa. voi sisältää työn arviointia tai jatkotutkimushaasteita
